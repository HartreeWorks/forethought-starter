# Thread from @TomDavidsonX

**Author:** Tom Davidson
**Date:** 2025-03-26
**URL:** https://x.com/TomDavidsonX/status/1904956748995182704

---

**1/11:** ðŸ“„New paper!

Once we automate AI R&D, there could be an intelligence explosion, even without labs getting more hardware.

Empirical evidence suggests the positive feedback loop of AI improving AI could overcome diminishing returns.

See ðŸ§µ.

**2/11:** A software intelligence explosion is where AI improves in a runaway feedback loop: AI makes smarter AI, which makes even-smarter AI etc.

AND this happens just via better AI software â€“ algorithms, data, post-training, etc. â€“ without needing more hardware.

Could that happen?

**3/11:** Each doubling of AI software efficiency will take more effort than the last â€“ that's diminishing returns.

But how much more effort?

If it takes >2X the effort, there's no software intelligence explosion.
If it takes <2X the effort, there is one.

![](https://pbs.twimg.com/media/Gm_AThUXcAAbLGV.jpg)

**4/11:** We can measure this empirically: we look at how much effort it took to double the efficiency of AI software once, and then at how much more effort it took to double it again.

Indeed, @EpochAIResearch have gathered empirical data on exactly this question from CS and ML.

**5/11:** Epoch estimate a parameter called the returns to AI software R&D, r.

If r > 1, we get a software intelligence explosion.
If r < 1, we don't.

![](https://pbs.twimg.com/media/Gm_BDrZXkAAnHn1.jpg)

**6/11:** Epoch find that r > 1 in various areas of AI, implying a software intelligence explosion is plausible.

![](https://pbs.twimg.com/media/Gm_BQJvXEAE6rFi.jpg)

**7/11:** No one's measured r for LLMs.

I bet r > 2 for LLMs, given the super-fast pace of algorithmic progress, esp combining gains from pre- and post-training.

That implies we'd comfortably get a software intelligence explosion if LLMs fully automate AI R&D.

![](https://pbs.twimg.com/media/Gm_Bd1bWwAA_d4m.jpg)

**8/11:** Estimates of r are conservative in some ways, aggressive in others.

Conservative: they only include efficiency gains, ignoring the benefits of smarter AI.

Aggressive: recent software progress may rely on increasing the supply of compute for experiments.

**9/11:** Most of the benefits of AI progress come from smarter AI, not efficiency.

So I would adjust naive estimates of r upwards by at least 2X

That puts my best guess at about r = 4 (for LLMs).

(r>1 implies software intelligence explosion)

**10/11:** I'd adjust r down to account for compute bottlenecks.

But as algorithms improve, we can run experiments more efficiently. And fine-tuning and scaffolding experiments don't use much compute.

If holding compute fixed reduces AI software progress by 3X, that still leaves r >1

**11/11:** When all's said and done, a software intelligence explosion is plausible enough that we need to be prepared.

See full paper: https://t.co/aigbiia2Kr
