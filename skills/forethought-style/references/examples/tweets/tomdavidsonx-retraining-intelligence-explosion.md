# Thread from @TomDavidsonX

**Author:** Tom Davidson
**Date:** 2025-03-28
**URL:** https://x.com/TomDavidsonX/status/1905622840453345510

---

**1/6:** Will the need to retrain AI models from scratch block a software intelligence explosion?

I argue that it won't, see ðŸ§µ

![](https://pbs.twimg.com/media/GnIfrmqXQAA_Aha.jpg)

**2/6:** A basic theoretical analysis suggests that if software progress would accelerate without retraining, it'll still accelerate with retraining.

**3/6:** The key idea:

When you double the efficiency of your AI training algorithms, you can "spend" that doubling EITHER on training more capable AI (with the same amt of compute), OR on making training faster (by using less compute).

![](https://pbs.twimg.com/media/GnIgexZWoAAw3ki.jpg)

**4/6:** So retraining will slow down a software intelligence explosion, but only by a small amount.

Though if acceleration was going to be extremely fast without retraining, this slowdown is bigger.

**5/6:** This work builds on my recent report on the software intelligence explosion with @daniel_271828

**6/6:** Link to full article:

https://t.co/PstUQTJm8m
