# Thread from @willmacaskill

**Author:** William MacAskill
**Date:** 2025-08-08
**URL:** https://x.com/willmacaskill/status/1953887265555616141

---

**1/28:** Should we expect widespread moral progress in the future?

In a new paper, Convergence and Compromise, @FinMoorhouse and I discuss this.

Thread.

![](https://pbs.twimg.com/media/Gx2aHOda4AAKl2f.jpg)

**2/28:** Given "no easy eutopia", we're very unlikely to get to a near-best future by default.

**3/28:** Instead, we'll need people to try really hard to bring it about: very proactively trying to figure out what's morally best - even if initially counterintuitive or strange - and trying to make that happen.

So: how likely is that to happen?

**4/28:** Maybe, in the future, most people will converge on the best understanding of ethics, and be motivated to act to promote good outcomes, whatever they may be. If so, then even if eutopia is a narrow target, we'll hit it anyway.

**5/28:** In conversation, I've been surprised by how many people (even people with quite anti-realist views on meta-ethics) have something like this view,

**6/28:** and therefore think that, even if the whole world were ruled by a single person, there would be quite a good chance that we'd still end up with a near-best future.

**7/28:** At any rate, Fin and I think such widespread convergence is pretty unlikely.

First, current moral agreement and seeming moral progress to date is weak evidence for the sort of future moral convergence that we'd need.

**8/28:** Present consensus is highly constrained by what's technologically feasible, and by the fact that so many things are instrumentally valuable: health, wealth, autonomy, etc. are useful for almost any terminal goal, so it's easy to agree that they're good.

**9/28:** This agreement could disappear once technology lets people optimise directly for terminal values (e.g., pure pleasure vs. pure preference-satisfaction).

**10/28:** Compared to how much we might reflect (and need to reflect) in the future, we've reflected very little, to date. What might seem like small differences, today, could become enormous after prolonged moral exploration, especially among beings who can radically self-modify.

**11/28:** Two people could be close to each other if they started off in the same spot and walked 10 metres in any direction. But if they keep walking for 100 miles, even slight differences in direction would lead them to end up very far apart.

**12/28:** What's more, a lot of moral progress to date has come from advocacy and coalition-building among disenfranchised groups.

**13/28:** But that's not an available mechanism to avoid many sorts of future moral catastrophe - e.g. around population ethics, or for digital beings who will be *trained* to endorse their situation, whatever their actual condition.

**14/28:** Will the post-AGI situation help us? Maybe, but it's far from decisive.

**15/28:** Superintelligent advice can make us reflect much more, but people might just not adequately reflect on their values, might not do so in time, or might deliberately avoid reflection that threatens their identity, ideology, or self-interest.

**16/28:** And if people had different starting intuitions and endorse different reflective procedures, then they could end up with diverging opinions even with superintelligent help.

**17/28:** Sometimes people argue that material abundance will make people more altruistic: they'll get all their self-interested needs met, and the only preferences left to satisfy will be altruistic.

![](https://pbs.twimg.com/media/Gx2aSsMaoAAtqjR.png)

**18/28:** But people could have (or develop) nearly linear-in-resources self-interested preferences, e.g. for collecting galaxies like stamps.

**19/28:** (Billionaires put barely a greater fraction of their wealth towards charity, despite having many orders of magnitude more wealth than the middle class.)

**20/28:** Or they could act on misguided ideological preferences - this argument doesn't give a reason for thinking that they'll become enlightened.

**21/28:** A more general argument against expecting widespread, accurate, motivational moral convergence is this:

**22/28:** If moral realism is true, the correct ethics will probably be quite alien; people might resist learning it; and even if they do, they might have little motivation to act in accordance with it.

**23/28:** If anti-realism is true, the level of convergence that we'd need seems improbable. Ethics has many "free parameters" (e.g. even if you're a hedonistic utilitarian - what exact sort of experience is best?),

**24/28:** and on anti-realism there's little reason to think that differing reflective procedures would end up in the same place.

**25/28:** Then, finally, we might not get widespread, accurate, motivational moral convergence because of "blockers".

**26/28:** Evolutionary dynamics might guide the future, rather than deliberate choices.

False but memetically powerful ideas might become dominant.

Or some early decisions could lock in later generations to a more constrained set of futures to choose between.

**27/28:** Putting this all together, widespread, accurate, motivational convergence seems unlikely to me.

But in our paper, we also discuss another path to a truly great future - via moral trade. I'll post on that soon.

**28/28:** https://t.co/GHJujgK1tE
