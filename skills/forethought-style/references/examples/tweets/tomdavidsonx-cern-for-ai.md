# Thread from @TomDavidsonX

**Author:** Tom Davidson
**Date:** 2025-03-21
**URL:** https://x.com/TomDavidsonX/status/1903076558593556856

---

**1/9:** There's recently been talk about "CERN for AI" and "Manhattan project for AI". The idea is to combine many western AI companies together into one centralized AGI project.

But would centralization be good? We analyzed its effects on AI risk, see ðŸ§µ

![](https://pbs.twimg.com/media/GmkVbdeXoAAvdRF.jpg)

**2/9:** Overall, we're skeptical of centralization.

Racing and infosecurity give weak reasons to centralize, but concentration of power gives a strong reason not to.

![](https://pbs.twimg.com/media/GmkVjh4WwAApdz2.jpg)

**3/9:** Centralization would reduce risky racing between (western) AI companies.

But it would increase racing with China, who might be scared by an AI project led by the US government.

**4/9:** Centralization would likely make it harder to steal model weights.

But it could also motivate foreign adversaries to try harder to steal the weights.

**5/9:** AGI will be a hugely powerful technology.

Power corrupts. A single project would have a monopoly on AGI, giving it unprecedented power.

Even a well-intentioned project might act badly if it's unaccountable. In the worst case, it could perform a coup.

![](https://pbs.twimg.com/media/GmkT1OhaEAA-4Xj.jpg)

**6/9:** A well-designed project could likely avoid risks from concentration of power.

Power within the project could be broadly distributed, intermediary companies could sell access to AI, and the project could make commitments to share benefits broadly.

**7/9:** Equally, a well-designed governance regime for multiple projects could likely avoid risks from racing and infosecurity.

**8/9:** We're excited for people to push for a great centralized project, and for great governance of multiple projects.

However many projects we end up with, it will be really important to actively manage risks from concentration of power.

**9/9:** Of course, there's plenty of room for reasonable disagreement here. This stuff is complicated! See more analysis here: https://t.co/WqOpCFraY3
